{"cells":[{"metadata":{"collapsed":true},"cell_type":"markdown","source":["# Overview\n","\n","In this notebook we'll explore a typocal workflow for applying data science to a problem and creating a predictive model. \n","\n","Though the process of applying data science to problems can cary somewhat from a problem to another problem, the jourey to AI follows the following steps - which we call the AI ladder - at its core.\n","\n","1. Collect\n","2. Organize\n","3. Analyze\n","4. Infuse\n","\n","In this notebook, we have selected some of the most common activities from the AI ladder and take you through them. specifically, this is the workflow that we will be covering.\n","\n","\n","### Notebook's workflow\n","\n","- Prepration\n","    1. Problem Definition\n","    2. Goal Setting\n","    3. Data Collection\n","- Data Prepration\n","    4. Data Exploration\n","    5. Data Cleaning\n","    6. Data Wrangling\n","- Data Analysis\n","    7. Data Visualization and insight\n","    8. Model Training\n","   \n","\n","   "]},{"metadata":{},"cell_type":"markdown","source":["## Tools we will use\n","\n","Throughout this notebook, you will be learn to use a few different python libraries. We introduce those tools here and import them to the project. To learn more about each one, you can click on their names:\n","\n","- [`matplotlib`](https://matplotlib.org/): A library for creating static, animated, and interactive visualizations.\n","- [`seaborn`](https://seaborn.pydata.org/): Another visualization library based on `matplotlib` that provides simpler higher-level pltting tools\n","- [`pandas`](https://pandas.pydata.org/docs/user_guide/10min.html): a library for data manipulation and analysis.\n","- [`numpy`](https://numpy.org/): A library library for working with arrays, matrices, and linear algebra\n","- [`sklearn`](https://scikit-learn.org/stable/getting_started.html): Scikit-learn is a library that provides machine learning algorithms and other relavant tools.\n"]},{"metadata":{},"cell_type":"code","source":["# Let's import these libraries\n","# Note that we give some of them short names for ease of use\n","\n","# Visualization\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Data Analysis\n","import pandas as pd\n","import numpy as np\n","import sklearn"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## 1. Prepration\n","\n","In this section, we takes the steps neccessary to set up our problem such that we can stay focused and achive the results that we need. "]},{"metadata":{},"cell_type":"markdown","source":["### 1.1 Problem Definition\n","\n","The first step to successfully apply machine learning to an AI problem is to clearly define the problem and set goals. \n","\n","**The problem**\n","\n","In this case the problem that we are trying to solve is:\n","> Given we know for a subset of Titanic passengers, who survived the disaster and who didn't, train a predictive model to predict which of the remaining passengers also survived with and optimize for accuracy.\n","\n","**Background Information**\n","\n","It is also helpful to have an understanding of the problem we are solving as well. This would help us when putting together our hypothesis when exploring the data and when we engineer new features that could possibly help our predictive models. \n","\n","For those who are not familiar with the event, take a look at the [Wikipedia](https://en.wikipedia.org/wiki/Titanic) article on the event pay particular attention to the \"[Survivors and victims](https://en.wikipedia.org/wiki/Titanic#Survivors_and_victims)\" section. That should provide some insight to the kinds patterns you might want to explore in the data."]},{"metadata":{},"cell_type":"markdown","source":["## 1.2 Goal Setting\n","\n","The next step is to define the goal we want to achive. In this case, we will set the followig goals to address our problem definition\n","\n","1. Exploring the Data\n","2. Cleaning the Data\n","3. Wrangling the Data\n","4. Classifying the Data\n","5. Visalizing the results\n","\n","The next steps are all focused around the following goals\n"]},{"metadata":{},"cell_type":"markdown","source":["## 1.3 Data Collection\n","\n","The last step before we start working with the data, is collecting the data. In this case we can simply load the data in, however, in more real-world usecases, one might need to use data mining processes or get the appropriate approvales before the data can be used. \n","\n","For our purpose, you can load the data using the [`read_csv`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) method of the [`pandas`](https://pandas.pydata.org/docs/user_guide/10min.html) library."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Import the data\n","df_data_1 = pd.read_csv(r'https://raw.githubusercontent.com/IBM/python-and-analytics/master/data/titanic.full.csv', na_values='?')"]},{"metadata":{},"cell_type":"markdown","source":["Notice the `na_values='?'` within the prenthesis after body. That's an additional argument that we passed into the `read_csv()` function of pandas to specify `?` in our datasets denotes blanks. You might need to adjust this for your own datasets."]},{"metadata":{},"cell_type":"markdown","source":["Our data is now loaded into our notebook in a `dataframe`. Think of a `dataframe` as a table in python which has rows, columns, and allows you to manipulate the data. We will look at some of these data manipulations below."]},{"metadata":{},"cell_type":"markdown","source":["For simplicity, let's call our data `df`.\n","\n","<font color='red'><b>IMPORTANT:</b></font> If your auto-import named the data differently, adjest the number on the right side of `df_data_1` accordingly\n"]},{"metadata":{},"cell_type":"code","source":["df = df_data_1"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Finally, we need to understand our data. For that, here is a quick description of what each column entails:\n","\n","| Column    | Description                                                          |\n","| :-------- | :------------------------------------------------------------------- |\n","| Pclass    | Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)                          |\n","| survival  | Survival (0 = No; 1 = Yes)                                           |\n","| name      | Name                                                                 |\n","| sex       | Sex                                                                  |\n","| age       | Age                                                                  |\n","| sibsp     | Number of Siblings/Spouses Aboard                                    |\n","| parch     | Number of Parents/Children Aboard                                    |\n","| ticket    | Ticket Number                                                        |\n","| fare      | Passenger Fare (British pound)                                       |\n","| cabin     | Cabin                                                                |\n","| embarked  | Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton) |\n","| boat      | Lifeboat                                                             |\n","| body      | Body Identification Number                                           |\n","| home.dest | Home/Destination                                                     |"]},{"metadata":{},"cell_type":"markdown","source":["At this point we are done with aquiring the data and definig our problem, and we are ready to move on to the Data Prepration step."]},{"metadata":{},"cell_type":"markdown","source":["## Data Prepration"]},{"metadata":{},"cell_type":"markdown","source":["### 2.1. Data Exploration\n","\n","Before we start cleaning our dataset, we need to understand our data. Let's do that by first visualizing the data and then perform simple statistcs on it."]},{"metadata":{},"cell_type":"markdown","source":["#### Using a Pandas Dataframe\n","\n","First let's take a look at our `dataframe` to become familiar with operations that we can perform on it, as well as what is inside it.\n","\n","##### Vieweing your data"]},{"metadata":{},"cell_type":"code","source":["# Let's print our DataFrame\n","df"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Next, let's look at the data types in each colum (aka. feature). It is always of interest to know what numerical, categorical, and text features.\n","\n","Here are some common data types:\n","\n","- `float`: Numbers with decimals\n","- `int`: Integers. Numbers without any decimals\n","- `object`: Textual values\n","- `category`: Categorical data\n","- `bool`: logical `True` or `False` values\n","- `datetime64`: A format for storing dates"]},{"metadata":{},"cell_type":"code","source":["df.dtypes"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Looks like some of the features are incorrectly picked as strings (`Objects`). We will take a look at those next and fix them in the data cleaning section. For now, let's contiue exploring the our `dataframe`"]},{"metadata":{},"cell_type":"markdown","source":["Next, let's see what methods our dataframe supports. To list these, in the cell below type `df.` and then press the `TAB` button on your keyboard. You should see a list of methods or attributes available."]},{"metadata":{},"cell_type":"code","source":["# type df. and press TAB at the end of the following line and explore the options\n","# df.<TAB>\n","\n"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Note that you also see the feature (column) names as things that you can call on `df`. That is the easiest way to pull out a column for manipulation, or vieweing. For example:"]},{"metadata":{},"cell_type":"markdown","source":["`describe()` anad `.info()` shows quick summary of your dataframe."]},{"metadata":{},"cell_type":"code","source":["df.describe()"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"code","source":["df.info()"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["`.T` quickly transposes the table."]},{"metadata":{},"cell_type":"code","source":["df.T"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["`.sort_index()` and `.sort_values()` sort your table."]},{"metadata":{},"cell_type":"code","source":["df.sort_index(axis=1)"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"code","source":["df.sort_values(by='age')"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["##### Selecting parts of the data\n","\n","You can find many ways of selecting your data in the [documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html#selection-by-label). Let's explore some of them together."]},{"metadata":{},"cell_type":"code","source":["# Column Selection\n","\n","# df.fare \n","df[\"fare\"]"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"code","source":["# Row Selections by row number\n","\n","df[0:4]"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"code","source":["# Row selection by another feature value\n","# Read \"Select df where age was over 5\"\n","\n","df[df[\"survived\"]==1]"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"code","source":["# Selecting by row and column\n","# use the df.loc[rows , [column1, column2, ...] ]\n","# for instance\n","\n","df.loc[0:3, [\"name\", \"sex\", \"age\"]]"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["##### setting values\n","\n","There are many ways to set values in your table which you can find in the [documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html#setting)\n","\n","You can, for instance, use their position, value, a condition about their value (eg. > 0), a condition on a different column(feature) (eg. if not survived, set age to 0!), etc... \n","\n","Let's explore some of them together"]},{"metadata":{},"cell_type":"code","source":["# let's start by making a copy of of df\n","# note that simply assisgning a different name wouldn't be enough (df2 = df). This would mean df and df2 point to the same table in the memory!\n","# The right way of making a copy is using the copy() method. \n","\n","df2 = df.copy()"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"code","source":["# Let's replace the '?' values in the age field with np.NaN (Not a Number) which is an actual value suppported by our table\n","df2[df[\"age\"]=='?'] = np.NaN\n","df2"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"code","source":["# Next, let's see what would happen if we dropped every row that has any NaN (blank) values.\n","\n","df2.dropna(how=\"any\")"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Interesting... Nothing remained! \n"," \n","Looks like every row has some unknowns. We shall do some more specific cleaning in our DataCleaning step.\n"," \n","For now, since `dropna()` doesn't perform its operation [`inplace`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html) and since we didn't assign the output of `df2.dropna(how=\"any\")` back to `df2`, the actual content of `df2` is unmodified, so we can move on. Let's verify `df2` is untouched."]},{"metadata":{},"cell_type":"code","source":["# Checking df2 is unmodified\n","df2"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["#### Operating on the data \n","\n","Now it's time for some exploration! You can see many of the possible operations in the [documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/basics.html#basics-binop). Here we will take a look at some of them to become familiar with the syntax."]},{"metadata":{},"cell_type":"markdown","source":["`.mean()`, `.median()`, `.max()`, `min()`, `skew()` are some of the basic statistical operations you can use. You can use it on the whole table, inidividual rows, or your selected data"]},{"metadata":{},"cell_type":"code","source":["df.mean()"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"code","source":["df[\"age\"].median()"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"code","source":["# Remember to select data conditionally you can do df[(condition1) & (condition2) & ...]\n","\n","female_survivors = df[ (df[\"sex\"]=='female') & (df[\"survived\"] == 1) ]\n","male_survivors = df[(df[\"sex\"]=='male') & (df[\"survived\"] == 1)]\n","\n","f_age_median = female_survivors[\"age\"].median()\n","m_age_median = male_survivors[\"age\"].median()\n","\n","print(\"Age medians f=\", f_age_median, \"m=\", m_age_median)"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"code","source":["f_age_skew = female_survivors[\"age\"].skew()\n","m_age_skew = male_survivors[\"age\"].skew()\n","\n","print(\"Age skew f=\", f_age_skew, \"m=\", m_age_skew)"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["We can see that there is signiticant age difference between male and female survivors and the skew in the data is different. So we will make sure to explore this further when we are visualizing the data. For now, you can explore the data even more with the statistical methods available before moving on."]},{"metadata":{},"cell_type":"markdown","source":["At this point we are familiar enough with the dataframes that we can move on to the visualization section. Of course, there are many other operations that we haven't covered yet in the interest of time such as joining tables, grouping items together, pivoting, timesries, etc. You are welcome to explore these operations in the `pandas` [documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html#setting)."]},{"metadata":{},"cell_type":"markdown","source":["#### Visual Exploration (matplotlib and seaborn)\n","\n","Now that we know how to use a dataframe, we can move to creating exploratory plots using seaborn and matplotlib. You can find extensive information on how to make plots with these tools online. Here, we will cover some of the most commonly used features."]},{"metadata":{},"cell_type":"markdown","source":["##### Histograms\n","\n","Let's begin by exploring the survial rates based on age on a histogram, we will use the `plt.hist()` function. \n","\n","Remember that `plt` was the short name we assigned `matplotlib.pyplot` when we imported it."]},{"metadata":{},"cell_type":"code","source":["plt.hist(df[df[\"survived\"] == 1][\"age\"])"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Interesting... looks like young children had high survival rates. Let's explore this more by increasing the number of bins."]},{"metadata":{},"cell_type":"code","source":["plt.hist(df[df[\"survived\"] == 1][\"age\"], bins = 20)"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Let's look at a more advanced syntax to simplify our visualization. here we will explore the survival by `pclass` by `age`. "]},{"metadata":{},"cell_type":"code","source":["# Create a seabord grid of empty plots with as many rows as unique \"pclass\" and columns as \"survived\" values\n","grid = sns.FacetGrid(df, row='survived', col=\"pclass\")\n","\n","# apply plt.hist on the 'age' column of each selected data in the grid. \n","grid.map(plt.hist, 'age', bins = 20)\n","\n","# So the first line splits the data by 'survived' and 'pclass' columns, and then grid.map applies a plt.hist(..., bins = 20) on the age column."],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Let's do the same for survivors based on age and sex as well"]},{"metadata":{},"cell_type":"code","source":["# Create a seabord grid of empty plots with as many rows as unique \"pclass\" and columns as \"survived\" values\n","grid = sns.FacetGrid(df, row='survived', col='sex')\n","\n","# apply plt.hist on the 'age' column of each selected data in the grid. \n","grid.map(plt.hist, 'age', bins = 20)"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["<b>Observation</b>\n","\n","This should provide more intuition on who survived and who didn't. It appears:\n","- pclass 1 had the least passengers, but most survived\n","- pclass 3 had the most passengers, most didn't survive\n","- most femakes survived while most men didn't\n","- most children and all elderly survived\n","- ...\n","\n","<b>Conclusion: </b>\n","\n","From this, we get the intuition that there might be a good corrolation between age, sex, pclass and weather someone survived or not. So these are features that we might consider for our model training."]},{"metadata":{},"cell_type":"markdown","source":["Next let's learn how to use `plt.pointplot` and `plt.barplot` to explore corrolations in categorical and numerical features.\n","\n","First, we will explore if the port of boarding (`embark`) seems to have a corrolation with the survival rate"]},{"metadata":{},"cell_type":"code","source":["grid = sns.FacetGrid(df, col='embarked')\n","grid.map(sns.pointplot, 'pclass', 'survived', 'sex')\n","grid.add_legend()"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["It appears that port of entry has an impact on whether males or females survived, so we should definitely consider adding this to our model. Next let's see if those who paid more, were more likely to survive. For this, we will use a `plt.barplot`"]},{"metadata":{},"cell_type":"code","source":["grid = sns.FacetGrid(df, row=\"survived\", col='embarked')\n","grid.map(sns.barplot, 'sex', 'fare')\n","grid.add_legend()"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["It also appears that, thought not as significant as other features, the paid fare played a role on weather someone survived or not. And this seems to be impacted by the port of entry as well as their gender. So let's consider `fare` for our model training as well."]},{"metadata":{},"cell_type":"markdown","source":["<b>Exercise:</b>\n","\n","This is where I courage you to explore your assumptions about the data and see if you can find correlations that could indicate your assumptions are correct or not."]},{"metadata":{},"cell_type":"code","source":["# Room for exercise\n","\n"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["This brings us to the end of our data exploration, we are now ready to clean and wrangle our data before we move on to model training."]},{"metadata":{},"cell_type":"markdown","source":["### 5. Data Cleaning\n","\n","Let's clean our data, remove what we don't need, and fill in the blanks first"]},{"metadata":{},"cell_type":"markdown","source":["#### Dropping unimportant features.\n","\n","While we can't know for sure if a feature is useful or not in the begining, we can get a good feeling about it based on our data exploration. It is a good practice to start with fewer features and slowly add more since fewer features means faster model training and iteration time.\n","\n","In this case, based on our oversvations in the previous section, we are going to keep `pclass`, `age`, `sex`, `embarked`, and `fare`. Additionally, we would see corrolations from the following if we explored them, so we will keep those as well: `name`, `sibsp`, `parch`\n","And of course our taget column: \"survived\""]},{"metadata":{},"cell_type":"code","source":["df = df.loc[:, [\"pclass\", \"age\", \"sex\", \"embarked\", \"fare\", \"name\", \"sibsp\", \"parch\", \"survived\"]]\n","df"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Since most Machine Learning models cannot deal with blank data, let's see which columns have blank values and address those. For this we use the `.isnull()` function that puts a true/false on each cell indicating if it is blank, and then we use the `.sum()` function on the result to sum up how many trues (blanks) are in each column."]},{"metadata":{},"cell_type":"code","source":["df.isnull().sum()"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Let's start with `age`. We shall replace the blank ones with a guessed value. Based on your knowledge of the domain, you can choose different methods for gessing blank numbers, here we can simply fill in the age with the the age median and move on. \n","\n","Of course, this is a very simplistic method, and introdces unwanted error. A better approach would be to guess the age based on the median of age in each group separated based on `sex`, `pclass`, `embarked`. But that is out of the scope of this introductory notebook.\n","\n","`.fillna()` is used to fill in the NaN values."]},{"metadata":{},"cell_type":"code","source":["# Let's get the Mode of age\n","age_mode = df['age'].mode()\n","age_mode"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"code","source":["# Since the returned Mode is a Series, we get the first and only element so we have a scalar number to use\n","age_mode = age_mode[0]\n","\n","# Then we can use the fillna()\n","df[\"age\"] = df[\"age\"].fillna(age_mode)"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"code","source":["# Let's verify we have no more NaN's in the age column\n","df[\"age\"].isnull().sum()"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["<font color='red'><b>Exercise:</b></font> Do the same for the embarked and fare columns."]},{"metadata":{},"cell_type":"code","source":["# Fill NaN embarked values with the mode of that column\n","\n","\n","# Fill NaN fare value with the mode of the column\n","\n"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Our Data is now relatively clean, so we can move on to the next step."]},{"metadata":{},"cell_type":"markdown","source":["### 6. Data Wrangling\n","\n","Many Machine Learning algorithems operate on numberical values only, so we need to convert our textual and categorial fields to numbers"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Let's see the types first\n","df.dtypes"]},{"source":["One way to do this is to replace each value with a number that you want. So something like this:\n","\n","```python\n","df['sex'] = df['sex'].map( {'female': 1, 'male': 0} )\n","df['sex'] = df['sex'].astype(int)\n","```\n","\n","However, if you don't care about which number represents which column, there is a simpler way using `.astype('category')` and `cat.codes`"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Convert the column type to categorical\n","df[\"sex\"] = df[\"sex\"].astype('category')\n","df.dtypes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Replace the categorical column with numberical codes\n","df[\"sex\"] = df[\"sex\"].cat.codes\n","df.dtypes"]},{"source":["Next, Let's do the same for the other categorical columns too."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[\"pclass\"] = df[\"pclass\"].astype('category').cat.codes\n","df[\"embarked\"] = df[\"embarked\"].astype('category').cat.codes\n","df[\"survived\"] = df[\"survived\"].astype('category').cat.codes\n","\n","# Let's check the head to see if it looks ok\n","df.dtypes"]},{"source":["Next, let's take care of the `name` column.\n","\n","Perhaps the titles of the passengers would have some corrolation with their survival. Let's see."],"cell_type":"markdown","metadata":{}},{"metadata":{},"cell_type":"code","source":["# Extract Titles from name\n","\n","# Here's what we are doing\n","# Left handside: create a new column called titles\n","# Right handside: \n","#   .str: treat the names as string\n","#   .extract: extract the part of string that matches our regula rexpression ' ([A-Za-z]+)\\.'\n","df['title'] = df[\"name\"].str.extract(' ([A-Za-z]+)\\.')\n","df['title']"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Regular Expressions: Also called regex, regular expressions are a syntax that describe textual patterns. You can read this tutorial on [python regex](https://docs.python.org/3/howto/regex.html) to learn more.\n","\n","Specifically, here our regex ` ([A-Za-z]+)\\.` means to find any word pattern that starts with a space(` `), ends with a dot (`\\.`) and has one or more `+` upper or lower case letters `[Az-az]`. \n","\n","[Click Here](https://regexr.com/5n4td) to see a visual description of what this regex does or create your own."]},{"source":["Now that we have our titles, let's break them down to common ones and rare ones since the specific title probably won't have much value, but rather weather it's a generic or rare one. "],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Let's see the unique titles:\n","\n","df[\"title\"].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Let's fix the typos\n","df[\"title\"] = df[\"title\"].replace('Mlle', \"Miss\")\n","df[\"title\"] = df[\"title\"].replace('Ms', \"Miss\")\n","df[\"title\"] = df[\"title\"].replace('Mme', \"Miss\")\n","df[\"title\"] = df[\"title\"].replace('', \"None\")\n","# Let's replace the rare ones with keyword 'rare'\n","# Here we are selecting the rare titles as anything not in a list\n","df[\"title\"][~df[\"title\"].isin([\"Mr\", \"Miss\", \"Mrs\", \"Master\"])] = \"Rare\"\n","\n","df[\"title\"].value_counts()\n"]},{"source":["Now let's see if our hypothesis seems resonable that rare titles have different chance of survival"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.barplot(x='title', y='survived', data=df)"]},{"metadata":{},"cell_type":"markdown","source":["There seems to be a corrolation between the title and survival rate. Even within the same gender (for instance male), the title seems to make a difference in the survival rate.\n","\n","So let's keep the new `title` column and drop the `name`.\n","\n","`.drop()` is used for this and we use `axis=1` to indicate it is a colum that we are dropping not a row"]},{"metadata":{},"cell_type":"code","source":["df = df.drop(\"name\", axis=1)\n","df.head()"],"execution_count":null,"outputs":[]},{"source":["Finally, let's replace the Title column with numbers and we are ready to train own Machine Learning Model"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# we will do this in steps so it's easier to follow\n","titles = df[\"title\"].astype('category').cat.codes\n","titles_as_category = titles.astype('category')\n","title_codes = titles_as_category.cat.codes\n","\n","# Assign the title codes \n","df[\"title\"] = title_codes\n","\n","# Print the top of the table to check\n","df.head()"]},{"source":["## Data Analysis\n","\n","Now that we have collected, cleaned, and wrangled our data, it is time to build our predictive model. Note that there are different families (classes) of predictive models (supervised, unsupervised, neural networks, ...), and each family has many models to select from. The choice of algorithm is one that you make in a real-life scenario, experiment and see the results of your selected technique, and then decide how to proceed: explore another family/model, keep the one you have trained, or create a hybrid (bagging, boosting, ensemble, ...) to capitalize on the strength of multiple models and families.\n","\n","In a real-life scenario, you would want to do some research on each model class, learn their strengths, weeknesses, input/output format, and then pick the most promissing ones based on your knowledge of the problem that you are solving.\n","\n","To keep this notebook within our timelimit, we will explore only one <b>\"classifier\"</b> model from the <b>\"supervised learning\"</b> class, called <b>\"Decision Tree\"</b>."],"cell_type":"markdown","metadata":{}},{"source":["### What is a Decision Tree\n","\n","From scikit-learn's documentation:\n","\n","> Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation.\n","\n","Simply put, this model is a tree-like set of decisions that finally lead to a prediction. \n","\n","As for its strengths here are some of the ones that relate to our problem today from scikit-learn's documentation:\n","\n",">\n","    Some advantages of decision trees are:\n","        - Simple to understand and to interpret. Trees can be visualised.\n","        - Requires little data preparation. Other techniques often require data normalisation, dummy variables need to be created and blank values to be removed. Note however that this module does not support missing values.\n","        - Able to handle both numerical and categorical data. However scikit-learn implementation does not support categorical variables for now. Other techniques are usually specialised in analysing datasets that have only one type of variable.\n","        - Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic. By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret.\n","        - Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.\n","\n","And as for relavant weaknesses:\n",">\n","    The disadvantages of decision trees include:\n","        - Decision-tree learners can create over-complex trees that do not generalise the data well. This is called overfitting. Mechanisms such as pruning, setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.\n","        - Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.\n","        - Predictions of decision trees are neither smooth nor continuous, but piecewise constant approximations. Therefore, they are not good at extrapolation.\n","        - There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.\n","        - Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree.\n","\n","With these pros and cons in mind, Decisions Trees seem like a strong candidate for our problem, so let's use them and see how they perform.\n","\n","[Learn more](https://scikit-learn.org/stable/modules/tree.html)"],"cell_type":"markdown","metadata":{}},{"source":["### Packages used: scikit-learn\n","\n","We will learn and use scikit learn which implements user friendly methods that can perform the training and testing of an algorithm for us. There are other libraries out there that could do the same, however, they often require more in-depth knowledge of how the algorithms work internally to properly configure.\n","\n","A few concepts that we should learn before moving further:\n","\n","\n","#### training/validation and test split\n","\n","We often split our dataset into three sections (train/validate/test) often with ratios (50/25/25). This is however not always the case and in simplified scenatios, such as our usecase, one might only use a (train/validate) split with ratio of (70/30). The use of the \"test\" set is described below.\n","\n","- The training set is shows to the model so it learns the patterns to use for its predictions\n","- The validation set is then then used the true performance of the model. This helps find out if the model just memorized the input data, or if it can in-fact predict unseen data correctly (aka. [generalize](https://en.wikipedia.org/wiki/Generalization_error)).\n","- The test set is used when the validation error is used for selecting the best model or when performing [hyper-parameter optimization](https://en.wikipedia.org/wiki/Hyperparameter_optimization)). In those cases, because we are using the error rate from the validation set to pick a best model, our validation error rate will be smaller than the true error rate of the model. Therefore, we need a set of data which we never used to optimize or pick our model to give us an unbiased error rate for our model.\n","\n","[Learn More](https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets)\n","\n","Since in this case we are just using one model (no model selection), and once trained we aren't tuning it any further (no Hyperparameter_optimization), we can skip the test set and just use a 70/30 train/validate split.\n","\n","#### Model fit\n","\n","It is important how a models prediction fits the data. If the model is not adequetly predicting the output based on the input in the training phase, it is called underfit. In an underfit case, the model will also perform poorly on the evaluation set.\n","\n","On the other hand, if a model performs amazingly on the training set (think 99% accurate) but poorly on the validation set, it is most likely overfit. You can think of this as a model that has basically memorized the training set and therefore is incapable of predicting well on unseen data. \n","\n","If a model is neither underfit nor overfit, it is called balanced. This is the place we want to be in and this is why we keep unseen data (validation and test splits). We use those unseen data to detect if our model is overfitting the data.\n","\n","[Learn More](https://en.wikipedia.org/wiki/Overfitting)\n","\n","#### Uppercase `X` and lowercase `y`\n","\n","There is a tradition where uppercase `X` denotes the input data to a model and the lowercase `y` the labels or values that it is expected to learn. That is, each set (train/validate/test) is broken into `X` and `y` where `y` is the predicted column and `X` is all the other columns.\n","\n","\n","With that, let's split our data, perform our training, and see how they perform on unseed data (validation)."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# import the libraries we need\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","\n","# Split target column our\n","X = df.drop(\"survived\", axis = 1)\n","y = df[\"survived\"]\n","\n","# perform the train/validation split\n","# test is our validation here since we won't further split out train set\n","# We specify the random state here for reprocucable results. \n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=20)\n","\n","# Let's check the shapes (table dimentions) to make sure it makes sense\n","X_train.shape, y_train.shape, X_test.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Exercise:\n","# Take a look at the data splits [X_train, X_test, y_train, y_test] and see if they make sense to you. You can explore, length, columns, dtypes, ...\n","\n"]},{"source":["<font colot='red'><b>Note:</b></font> \n","The following line won't work, once you see the error, read it try to debug the issue in the next cell, then move forward to see the solution."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Decision Tree\n","decision_tree = DecisionTreeClassifier()\n","\n","# Note: This won't work, once you see the error, read it try to debug the issue, then move forward to see the solution.\n","decision_tree.fit(X_train, y_train)\n"]},{"source":["Looks like something is wrong. Take a look at the ValueError message and use the cell below to explore and see if you can findout what is wrong. We left a small mistake intentionally to experience the debugging process"],"cell_type":"markdown","metadata":{}},{"source":["# Exrcise \n","# Explore the copy of our original table below and find out what is wrong\n","# Make sure not to modify to original df, and only work on the df2\n","df2 = df.copy()\n","\n","# You may explore df2 here\n"],"cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":["<b>Solution:</b>\n","\n","As the error says:\n","> Input contains NaN, infinity or a value too large for dtype('float32')\n","\n","So let's find the column with NaN and fill it with an average for that column.\n","\n","We will use the `.isnull()` to fill the table with true wherever there is a null, and then use the `.any()` to show if in each column there is any True (which would mean a `NaN`). You can run these steps one-by-one "],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","df.isnull().any()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Now let's count the NaN in fare column\n","# Note that `df[\"fare\"].isnull().sum()` wouldn't work as it would count any value, true and false! while sum counts true as 1 and false as 0.\n","df[\"fare\"].isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ok there is only one missing value, so let's replace it with the mean and move on.abs\n","fare_mode = df[\"fare\"].mode()[0]\n","df[\"fare\"] = df[\"fare\"].fillna(fare_mode)\n","\n","# and let's make sure there are no more nulls\n","df.isnull().any()"]},{"source":["Perfect, no more nulls, we can now train our model"],"cell_type":"markdown","metadata":{}},{"source":["# Redo our split with fixed data\n","X = df.drop(\"survived\", axis = 1)\n","y = df[\"survived\"]\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n","\n","# Decision Tree\n","decision_tree = DecisionTreeClassifier()\n","\n","# Train Model\n","decision_tree.fit(X_train, y_train)\n","\n","# Make predictions for validation set\n","y_pred = decision_tree.predict(X_test)\n","\n","# calculate models accuracy on training set\n","train_score = decision_tree.score(X_train, y_train)\n","test_score = decision_tree.score(X_test, y_test)\n","\n","# Print results\n","# We know this is a new syntax to format and round numbers to two decimal places within a string. \n","# You can learn more at https://www.w3schools.com/python/ref_string_format.asp\n","print(\"Trainins Score: {:.2f} validation score: {:.2f}\".format(train_score, test_score))"],"cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":["It looks like our decision tree is overfitting! It does very well with the training data 97% accuracy and performs poorly on unseen data (validation set). \n","\n","How to counter that is beyond the scope of this notebook, but you can begin by forcing the each branch of the tree to have at least 15 leaves. That is replacing the following line\n","```python\n","decision_tree = DecisionTreeClassifier()\n","```\n","with:\n","```python\n","decision_tree = DecisionTreeClassifier(min_samples_leaf=10)\n","```\n","\n","This should reduce overfitting and improve our accuracy to about 80%! "],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Tuning the Tree Parameters\n","decision_tree = DecisionTreeClassifier(min_samples_leaf=10)\n","\n","# Train Model\n","decision_tree.fit(X_train, y_train)\n","\n","# Make predictions for validation set\n","y_pred = decision_tree.predict(X_test)\n","\n","# calculate models accuracy on training set\n","train_score = decision_tree.score(X_train, y_train)\n","test_score = decision_tree.score(X_test, y_test)\n","\n","# Print results\n","print(\"Trainins Score: {:.2f} validation score: {:.2f}\".format(train_score, test_score))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.8.5-final","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}