{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Overview\n",
    "\n",
    "In this notebook, we'll explore a typical workflow for applying data science to a problem and creating a predictive model.\n",
    "\n",
    "Though the process of applying data science to problems can vary somewhat from one problem to another, the journey to AI follows the following steps - which we call the AI ladder - at its core.\n",
    "\n",
    "1. Collect\n",
    "2. Organize\n",
    "3. Analyze\n",
    "4. Infuse\n",
    "\n",
    "This notebook has selected some of the most common activities from the AI ladder and takes you through them. Specifically, this is the workflow that we will be covering.\n",
    "\n",
    "\n",
    "### Notebook's workflow\n",
    "\n",
    "#### [ยง Preparation](#Prepration)\n",
    "1. [Problem Definition](#1.-Problem-Definition)\n",
    "2. [Goal Setting](#2.-Goal-Setting)\n",
    "3. [Data Collection](#3.-Data-Collection)\n",
    "    \n",
    "#### [ยง Data preparation](#Data-preparation)\n",
    "4. [Data Exploration](#4.-Data-Exploration)\n",
    "    - [Dataframes Essentials](#Using-a-Pandas-Dataframe)\n",
    "        1. [Vieweing your data](#Vieweing-your-data)\n",
    "        1. [Data Types](#Data-Types)\n",
    "        1. [Basic Methods](#Basic-Methods)\n",
    "        1. [Selecting Data](#Selecting-parts-of-the-data)\n",
    "        1. [Setting Values](#setting-values)\n",
    "        1. [Manipulating the Data](#Operating-on-the-data)\n",
    "    - [Plotting Essentials](#Visual-Exploration-(matplotlib-and-seaborn))\n",
    "        1. [Histograms](#Histograms)\n",
    "        1. [Pin Plots](#Pin-Plots)\n",
    "        1. [Bar Plots](#Bar-Plots)\n",
    "5. [Data Cleaning](#5.-Data-Cleaning)\n",
    "6. [Data Wrangling](#6.-Data-Wrangling)\n",
    "\n",
    "#### [ยง Data Analysis](#Data-Analysis)\n",
    "7. [Splitting Data for Training](#7.-Splitting-Data-for-Training)\n",
    "8. [Model Training](#8.-Model-Training)\n",
    "   \n",
    "#### [ยง Conclusion](#Conclusion)\n",
    "9. [Final Challenge](#10.-Final-Challenge)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepration\n",
    "\n",
    "In this section, we take the steps necessary to set up our problem to stay focused and achieve the results that we need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools we will use\n",
    "\n",
    "A short note on the tools we will use. \n",
    "\n",
    "Throughout this notebook, you will learn to use a few different Python libraries. We introduce those tools here and import them to the project. To learn more about each one, you can click on their names:\n",
    "\n",
    "- [`matplotlib`](https://matplotlib.org/): A library for creating static, animated, and interactive visualizations.\n",
    "- [`seaborn`](https://seaborn.pydata.org/): Another visualization library based on matplotlib that provides simpler higher-level plotting tools.\n",
    "- [`pandas`](https://pandas.pydata.org/docs/user_guide/10min.html): A library for data manipulation and analysis.\n",
    "- [`sklearn`](https://scikit-learn.org/stable/getting_started.html): Scikit-learn is a library that provides machine learning algorithms and other relevant tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import these libraries\n",
    "# Note that we give some of them short names for ease of use\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Data Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Problem Definition\n",
    "\n",
    "The first step to successfully applying machine learning to an AI problem is defining the problem and setting goals. Let's solidify the problem statement and gather the necessary domain information.\n",
    "\n",
    "\n",
    "**The problem**\n",
    "\n",
    "In this case, the problem that we are trying to solve is:\n",
    "> Given a subset of Titanic passengers' survival status, train a predictive model that predicts which of the remaining passengers also survived. The model should be optimized for accuracy.\n",
    "\n",
    "**Background Information**\n",
    "\n",
    "It is also helpful to know the event that happened, that is, gather the domain knowledge. This would help us put together specific hypotheses when exploring the data and creating new features to help our predictive models.\n",
    "\n",
    "If you are unfamiliar with the event, look at the [Wikipedia](https://en.wikipedia.org/wiki/Titanic) article on the event paying particular attention to the \"[Survivors and victims](https://en.wikipedia.org/wiki/Titanic#Survivors_and_victims)\" section. That should provide some insight into the kinds of patterns you might want to explore in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Goal Setting\n",
    "\n",
    "The next step is to define the goal we want to achieve. In this case, we will set the following goals to address our problem definition\n",
    "\n",
    "1. Exploring the Data\n",
    "2. Cleaning the Data\n",
    "3. Wrangling the Data\n",
    "4. Training a classifier\n",
    "    \n",
    "The next steps are all focused on the following goals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Collection\n",
    "\n",
    "The last step before we start working with the data is collecting the data. In this case, we can load the data in from a file that we have prepared; however, in real-world use-cases, one might need to use data mining processes and get the appropriate approvals before using the data.  \n",
    "\n",
    "For our purpose, you can load the data using the [`read_csv`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) method of the [`pandas`](https://pandas.pydata.org/docs/user_guide/10min.html) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the data\n",
    "df = pd.read_csv(r'https://raw.githubusercontent.com/IBM/python-and-analytics/master/data/titanic.full.csv', na_values='?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the `na_values='?'` within the parenthesis after body. That's an additional argument that we passed into the `read_csv()` function of pandas to specify `?` in our datasets denotes blanks. You might need to adjust this for your datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is now loaded into our notebook in a `dataframe`. Think of a `dataframe` as a table in python with rows and columns. A `dataframe` allows you to manipulate the data. We will look at some of these data manipulations below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to understand our data. For that, here is a quick description of what each column entails and the first few lines of the table:\n",
    "\n",
    "| Column    | Description                                                          |\n",
    "| :-------- | :------------------------------------------------------------------- |\n",
    "| Pclass    | Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)                          |\n",
    "| survival  | Survival (0 = No; 1 = Yes)                                           |\n",
    "| name      | Name                                                                 |\n",
    "| sex       | Sex                                                                  |\n",
    "| age       | Age                                                                  |\n",
    "| sibsp     | Number of Siblings/Spouses Aboard                                    |\n",
    "| parch     | Number of Parents/Children Aboard                                    |\n",
    "| ticket    | Ticket Number                                                        |\n",
    "| fare      | Passenger Fare (British pound)                                       |\n",
    "| cabin     | Cabin                                                                |\n",
    "| embarked  | Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton) |\n",
    "| boat      | Lifeboat                                                             |\n",
    "| body      | Body Identification Number                                           |\n",
    "| home.dest | Home/Destination                                                     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First few lines of the table. \n",
    "# We'll learn more about the head() method later\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we are done with acquiring the data and defining our problem, and we are ready to move on to the Data Preparation step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prepration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Exploration\n",
    "\n",
    "Before we start cleaning our dataset, we need to understand our data. Let's do that by first visualizing the data and then perform simple statistcs on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a Pandas Dataframe\n",
    "\n",
    "First, let's take a look at our `dataframe` to become familiar with operations that we can perform on it and what is inside it.\n",
    "\n",
    "##### Vieweing your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print our DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This form of printing the `dataframe` takes a lot of space which is unnecessary since we manipulate the table with code.  So from here on, we will use `df.head()` to only show the top of the table instead. You can also use `df.tail()` if you want to see the end of a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try df.head() and df.tail() in this cell.\n",
    "# Notice that jupyter will show only one of them at a time. So if you type both, only the last one is shown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Types\n",
    "Next, let's look at the data types in each column (aka. feature). It is always of interest to know what numerical, categorical, and text features.\n",
    "\n",
    "Here are some common data types:\n",
    "\n",
    "- `float`: Numbers with decimals\n",
    "- `int`: Integers. Numbers without any decimals\n",
    "- `object`: Textual values\n",
    "- `category`: Categorical data\n",
    "- `bool`: logical `True` or `False` values\n",
    "- `datetime64`: A format for storing dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise\n",
    "# View the datatypes for each column\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like some of the features are incorrectly detected as strings (`Objects`). We will take a look at those next and fix them in the data cleaning section. For now, let's continue exploring our `dataframe.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Basic Methods\n",
    "\n",
    "Next, let's see what methods our dataframe supports. To list these, in the cell below, type `df.` and then press the `TAB` button on your keyboard. You should see a list of methods or attributes available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise\n",
    "# type df. and press TAB at the end of the following line and explore the options\n",
    "# df.<TAB>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you also see the feature (column) names as things that you can call on `df`. That is the easiest way to pull out a column for manipulation, or vieweing. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`describe()` and `.info()` show quick summary of your dataframe. Let's give them a try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise\n",
    "# Try df.describe()\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise\n",
    "# Try df.info()\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.T` quickly transposes the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise\n",
    "# Try df.T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.sort_index()` and `.sort_values()` sort your table. \n",
    "\n",
    "if you pass `axis=1` to these functions they will operate on the columns. That is, they will sort the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise\n",
    "# Try df.sort_index(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise\n",
    "# Try df.sort_values(by='age')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Selecting parts of the data\n",
    "\n",
    "Next, let's look at how to select parts of our table. We will use data selection extensively in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise:\n",
    "# Column Selection\n",
    "# try df.fare or df[\"fare\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise:\n",
    "# Row Selections by row number. The syntax is df[begining row : final row]\n",
    "# try df[0:4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise:\n",
    "# Conditional row selection\n",
    "# try the following. Reads \"Select df where survived value is one\"\n",
    "# try df[df[\"survived\"]==1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting by row and column\n",
    "# use the df.loc[rows , [column1, column2, ...] ]\n",
    "# try the following. Reads \"from df select rows 0 to 3 and columns name, sex, and age\"\n",
    "# try df.loc[0:3, [\"name\", \"sex\", \"age\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Learn More](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html#selection-by-label) about Data Selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### setting values\n",
    "\n",
    "There are many ways to set values in your table which you can find in the [documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html#setting)\n",
    "\n",
    "You can, for instance, use their position, value, a condition about their value (eg. > 0), a condition on a different column(feature) (eg. if not survived, set age to 0!), etc... \n",
    "\n",
    "Let's explore some of them together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we make a copy of our data, so the original dataset remains intact. To do this, use the `.copy()` method. \n",
    "\n",
    "<b>Note</b> that simply assigning a different name to your  `dataframe` wouldn't be enough (ie. `df2 = df`). This would mean `df` and `df2` point to the same table in the memory! That is if you would modify `df2`, `df` would also reflect those changes. \n",
    "\n",
    "The right way of making a copy is using the `.copy()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy\n",
    "df2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, let's see what would happen if we dropped every row that has any NaN (blank) values.\n",
    "\n",
    "df2.dropna(how=\"any\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting... Nothing remained! \n",
    " \n",
    "It looks like every row has some unknowns. We shall do some more specific cleaning in our data cleaning step.\n",
    " \n",
    "For now, since `dropna()` doesn't perform its operation [`inplace`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html) and since we didn't assign the output of `df2.dropna(how=\"any\")` back to `df2`, the actual content of `df2` is unmodified, so we can move on. \n",
    "\n",
    "Let's verify `df2` is untouched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking df2 is unmodified\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operating on the data \n",
    "\n",
    "Now it's time for some exploration! You can see many of the possible operations in the [documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/basics.html#basics-binop). Here we will take a look at some of them to become familiar with the syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.mean()`, `.median()`, `.max()`, `min()`, `skew()` are some of the basic statistical operations you can use. You can use it on the whole table, inidividual rows, or your selected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"age\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to select data conditionally you can do df[(condition1) & (condition2) & ...]\n",
    "\n",
    "female_survivors = df[ (df[\"sex\"]=='female') & (df[\"survived\"] == 1) ]\n",
    "male_survivors   = df[ (df[\"sex\"]=='male')   & (df[\"survived\"] == 1) ]\n",
    "\n",
    "f_age_median = female_survivors[\"age\"].median()\n",
    "m_age_median = male_survivors[\"age\"].median()\n",
    "\n",
    "print(\"Age medians f=\", f_age_median, \"m=\", m_age_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_age_skew = female_survivors[\"age\"].skew()\n",
    "m_age_skew = male_survivors[\"age\"].skew()\n",
    "\n",
    "print(\"Age skew f=\", f_age_skew, \"m=\", m_age_skew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a significant age difference between male and female survivors, and the skew in the data is different. So we will make sure to explore this further when we are visualizing the data. For now, you can explore the data even more with the statistical methods available before moving on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we are familiar enough with the dataframe that we can move on to the visualization section. Of course, there are many other operations that we haven't covered yet in the interest of time, such as joining tables, grouping items together, pivoting, time series, etc. You learn more about these operations by referring to the  [documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html#setting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visual Exploration (matplotlib and seaborn)\n",
    "\n",
    "Now that we know how to use a dataframe and select different parts of it, we can create experimental plots using seaborn and matplotlib. You can find extensive information on how to make plots with these tools online. Here, we will cover some of the most commonly used features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Histograms\n",
    "\n",
    "Let's begin by exploring the survival rates based on age on a histogram. We will use the `plt.hist()` function. \n",
    "\n",
    "Remember that `plt` was the short name we assigned `matplotlib.pyplot` when we imported it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`plt.hist()` is used to create histograms. You need to select one column to create a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[df[\"survived\"] == 1][\"age\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting... It looks like young children had high survival rates. Let's explore this more by increasing the number of bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[df[\"survived\"] == 1][\"age\"], bins = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Conclusion:</b> We can see that many people under the age of 4 and between age 70-80 survived the disaster. This indicates that there is some correlation between age and whether someone survived the disaster. Therefore, we make sure to include this feature in our model training later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a more advanced syntax to simplify our visualization. here we will explore the survival by `pclass` by `age`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a seaborn grid of empty plots with as many rows as unique values in \"pclass\" and columns as \"survived\" \n",
    "grid = sns.FacetGrid(df, row='survived', col=\"pclass\")\n",
    "\n",
    "# apply plt.hist on the 'age' column of each selected data in the grid. \n",
    "grid.map(plt.hist, 'age', bins = 20)\n",
    "\n",
    "# So the first line splits the data by 'survived' and 'pclass' columns, and then grid.map applies a plt.hist(..., bins = 20) on the age column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for survivors based on age and sex as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a seabord grid of empty plots with as many rows as unique \"pclass\" and columns as \"survived\" values\n",
    "grid = sns.FacetGrid(df, row='survived', col='sex')\n",
    "\n",
    "# apply plt.hist on the 'age' column of each selected data in the grid. \n",
    "grid.map(plt.hist, 'age', bins = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Observation</b>\n",
    "\n",
    "These plots provide more intuition on who survived and who didn't. It appears that:\n",
    "- pclass 1 had the least passengers, but most survived\n",
    "- pclass 3 had the most passengers, and most didn't survive\n",
    "- most females survived while most men didn't\n",
    "- most children and all elderly survived\n",
    "- ...\n",
    "\n",
    "<b>Conclusion: </b>\n",
    "\n",
    "From this, we get the intuition that there might be a good correlation between age, sex, pclass, and whether someone survived or not. So these are features that we might consider for our model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pin Plots\n",
    "Next, let's learn how to use `plt.pointplot` and `plt.barplot` to explore correlations in categorical and numerical features.\n",
    "\n",
    "First, we will explore if the port of boarding (`embark`) seems to correlate with the survival rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.FacetGrid(df, col='embarked')\n",
    "grid.map(sns.pointplot, 'pclass', 'survived', 'sex')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Conclusion: </b> It appears that port of entry has an impact on whether males or females survived, so we should definitely consider adding this to our model. \n",
    "\n",
    "##### Bar Plots\n",
    "\n",
    "Next let's see if those who paid more, were more likely to survive. For this, we will use a `plt.barplot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.FacetGrid(df, row=\"survived\", col='embarked')\n",
    "grid.map(sns.barplot, 'sex', 'fare')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Conclusion: </b> It also appears that though not as significant as other features, the paid fare played a role in whether someone survived or not. And this seems to be impacted by the port of entry as well as their gender. So let's consider `fare` for our model training as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise:</b>\n",
    "\n",
    "You are now well-equipped to explore your assumptions about this data. Let's see if you can find correlations that could indicate your assumptions are correct or not.\n",
    "\n",
    "You could give plots such as [heatmaps](https://seaborn.pydata.org/generated/seaborn.heatmap.html), [box plots](https://seaborn.pydata.org/generated/seaborn.boxplot.html), and [Joint Distribution Plots](https://seaborn.pydata.org/generated/seaborn.jointplot.html) a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Room for exercise\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This brings us to the end of our data exploration; we are now ready to clean and wrangle our data and then proceed to model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Data Cleaning\n",
    "\n",
    "Let's clean our data, remove what we don't need, and fill in the blanks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping unimportant features.\n",
    "\n",
    "While we can't know if a feature is useful or not initially, we can get a good feeling about it based on our data exploration. It is always a good practice to start with fewer features and slowly add more since fewer features mean faster model training and iteration time.\n",
    "\n",
    "In this case, based on our observations in the previous section, we are going to keep `pclass`, `age`, `sex`, `embarked`, and `fare`. Additionally, we would see correlations from the following if we explored them, so we will keep those as well: `name`, `sibsp`, `parch`\n",
    "And of course, our target column: \"survived.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:, [\"pclass\", \"age\", \"sex\", \"embarked\", \"fare\", \"name\", \"sibsp\", \"parch\", \"survived\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since most Machine Learning models cannot deal with blank data, let's see which columns have empty values and address those. For this, we use the `.isnull()` function that puts a true/false on each cell indicating if it is blank, and then we use the `.sum()` function on the result to sum up how many trues (blanks) are in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise\n",
    "# You can try df.isnull() by itself to see what the output looks like\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with `age` as it has the most blanks. We shall replace the blank ones with a guessed value. Based on your knowledge of the domain, you can choose different methods for guessing empty numbers; here, we can fill in the age median and move on. \n",
    "\n",
    "Of course, this is a very simplistic method and introduces unwanted error. A better approach would be to guess the age based on the median of age in each group separated based on `sex`, `pclass`, `embarked`. But that is out of the scope of this introductory notebook.\n",
    "\n",
    "`.fillna()` is used to fill in the NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the Mode of age\n",
    "age_mode = df['age'].mode()\n",
    "age_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since age_mode is a Series, we get the first and only element, so we have a scalar number to use\n",
    "age_mode = age_mode[0]\n",
    "\n",
    "# Then we can use the fillna()\n",
    "df[\"age\"] = df[\"age\"].fillna(age_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's verify we have no more NaN's in the age column\n",
    "df[\"age\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'><b>Exercise:</b></font> Do the same for the embarked column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise \n",
    "# Fill NaN embarked values with the mode of that column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Data is now relatively clean, so we can move on to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Data Wrangling\n",
    "\n",
    "Many Machine Learning algorithms operate on numerical values only, so we need to convert our textual and categorial fields to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the types first\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to do this is to replace each value with a number that you want. You would use the following syntax for that.\n",
    "\n",
    "```python\n",
    "df['sex'] = df['sex'].map( {'female': 1, 'male': 0} )\n",
    "df['sex'] = df['sex'].astype(int)\n",
    "```\n",
    "\n",
    "However, since we don't care about which number represents which column, there is a simpler way using `.astype('category')` and `cat.codes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the column type to categorical\n",
    "df[\"sex\"] = df[\"sex\"].astype('category')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the categorical column with numberical codes\n",
    "df[\"sex\"] = df[\"sex\"].cat.codes\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise \n",
    "# print the values inside the \"sex\" column to see if they are what you expected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, Let's do the same for the other categorical columns too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pclass\"] = df[\"pclass\"].astype('category').cat.codes\n",
    "df[\"embarked\"] = df[\"embarked\"].astype('category').cat.codes\n",
    "df[\"survived\"] = df[\"survived\"].astype('category').cat.codes\n",
    "\n",
    "# Let's check the head to see if it looks ok\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's take care of the `name` column.\n",
    "\n",
    "Perhaps the titles of the passengers would have some corrolation with their survival. Let's see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Titles from name\n",
    "\n",
    "# Here's what we are doing\n",
    "# Left handside: create a new column called titles\n",
    "# Right handside: \n",
    "#   .str: treat the names as string\n",
    "#   .extract: extract the part of string that matches our regular rexpression ' ([A-Za-z]+)\\.'\n",
    "# See the next cell for what a regular expression is.\n",
    "df['title'] = df[\"name\"].str.extract(' ([A-Za-z]+)\\.')\n",
    "df['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Regular Expressions</b>: Also called regex, regular expressions are a syntax that describes textual patterns. You can read this tutorial on [python regex](https://docs.python.org/3/howto/regex.html) to learn more.\n",
    "\n",
    "Specifically, here our regex ` ([A-Za-z]+)\\.` means to find any word pattern that starts with one space(` `), ends with one dot (`\\.`), and has one or more `+` upper or lower case letters `[Az-az]`. \n",
    "\n",
    "[Click Here](https://regexr.com/5n4td) to see a visual description of what this regex does or create your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our titles let's group the rare ones since the specific title probably won't have much value, but whether it's a generic or rare one might."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.count_values()` counts unique values in a column and shows you how many times each repeats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the unique titles:\n",
    "\n",
    "df[\"title\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.replace(x,y)` replaces all occurrences of `x` with `y` within the given column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's fix the typos\n",
    "df[\"title\"] = df[\"title\"].replace('Mlle', \"Miss\")\n",
    "df[\"title\"] = df[\"title\"].replace('Ms', \"Miss\")\n",
    "df[\"title\"] = df[\"title\"].replace('Mme', \"Miss\")\n",
    "df[\"title\"] = df[\"title\"].replace('', \"None\")\n",
    "# Let's replace the rare ones with keyword 'rare'\n",
    "# Here we are selecting the rare titles as anything not in a list\n",
    "df[\"title\"][~df[\"title\"].isin([\"Mr\", \"Miss\", \"Mrs\", \"Master\"])] = \"Rare\"\n",
    "\n",
    "df[\"title\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if our hypothesis seems reasonable that rare titles have a different chance of survival. To do this, we will make a bar plot using `sns.barplot` and see if there is a notable difference between the averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='title', y='survived', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a correlation between the title and survival rate. Even within the same gender (for instance, male), the title seems to make a difference in the survival rate.\n",
    "\n",
    "<b>Conclusion:</b> So let's keep the new `title` column and drop the `name`. There seems to be value to be gained from the title column.\n",
    "\n",
    "`.drop()` is used for this, and we use `axis=1` to indicate it is a column that we are dropping, not a row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"name\", axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's replace the Title column with numerical values and we are ready to train own Machine Learning Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will do this in steps so it's easier to follow\n",
    "titles = df[\"title\"].astype('category').cat.codes\n",
    "titles_as_category = titles.astype('category')\n",
    "title_codes = titles_as_category.cat.codes\n",
    "\n",
    "# Assign the title codes \n",
    "df[\"title\"] = title_codes\n",
    "\n",
    "# Print the top of the table to check\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "\n",
    "Now that we have collected, cleaned, and wrangled our data, it is time to build our predictive model. Note that there are different families (classes) of predictive models (supervised, unsupervised, neural networks, ...), and each family has many models to select from. The choice of algorithm is one that you make in a real-life scenario, experiment and see the results of your technique chosen, and then decide how to proceed:\n",
    "\n",
    "- Explore another family/model.\n",
    "- Keep the one you have trained.\n",
    "- Create a hybrid (bagging, boosting, ensemble, etc) to capitalize on multiple models' strength.\n",
    "\n",
    "In a real-life scenario, you would want to do some research on each model class, learn their strengths, weaknesses, input/output format, and then pick the most promising ones based on your knowledge of the problem that you are solving.\n",
    "\n",
    "To keep this notebook within our timelimit, we will explore only one <b>\"classifier\"</b> model from the <b>\"supervised learning\"</b> class, called <b>\"Decision Tree\"</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Decision Tree\n",
    "\n",
    "From scikit-learn's documentation:\n",
    "\n",
    "> Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation.\n",
    "\n",
    "Simply put, this model is a tree-like set of decisions that finally lead to a prediction. \n",
    "\n",
    "As for its strengths, here are some of the ones that relate to our problem today from scikit-learn's documentation:\n",
    "\n",
    ">\n",
    "    Some advantages of decision trees are:\n",
    "        - Simple to understand and to interpret. Trees can be visualised.\n",
    "        - Requires little data preparation. Other techniques often require data normalisation, dummy variables need to be created and blank values to be removed. Note however that this module does not support missing values.\n",
    "        - Able to handle both numerical and categorical data. However scikit-learn implementation does not support categorical variables for now. Other techniques are usually specialised in analysing datasets that have only one type of variable.\n",
    "        - Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic. By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret.\n",
    "        - Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.\n",
    "\n",
    "And as for relevant weaknesses:\n",
    ">\n",
    "    The disadvantages of decision trees include:\n",
    "        - Decision-tree learners can create over-complex trees that do not generalise the data well. This is called overfitting. Mechanisms such as pruning, setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.\n",
    "        - Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.\n",
    "        - Predictions of decision trees are neither smooth nor continuous, but piecewise constant approximations. Therefore, they are not good at extrapolation.\n",
    "        - There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.\n",
    "        - Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree.\n",
    "\n",
    "With these pros and cons in mind, Decisions Trees seem like a strong candidate for our problem, so let's use them and see how they perform.\n",
    "\n",
    "[Learn more](https://scikit-learn.org/stable/modules/tree.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages used: scikit-learn\n",
    "\n",
    "We will learn and use scikit learn, which implements user-friendly methods to perform the training and testing of an algorithm for us. There are other libraries out there that could do the same. However, they often require more in-depth knowledge of how the algorithms work internally to configure correctly.\n",
    "\n",
    "A few concepts that we should learn before moving further:\n",
    "\n",
    "\n",
    "#### training/validation and test split\n",
    "\n",
    "We often split our dataset into three sections (train/validate/test), often with ratios (50/25/25). This is, however, not always the case, and in simplified scenarios, such as our use-case, one might only use a (train/validate) split with a ratio of (70/30). We discuss the use of the \"test\" below.\n",
    "\n",
    "- The training set is shown to the model, so it learns the patterns to use for its predictions\n",
    "- The validation set is then used to assess the true performance of the model. This technique helps find out if the model just memorized the input data or if it can in-fact predict unseen data correctly (aka. [generalize](https://en.wikipedia.org/wiki/Generalization_error)).\n",
    "- The test set is used when the validation error is used for selecting the best model or when performing [hyper-parameter optimization](https://en.wikipedia.org/wiki/Hyperparameter_optimization)). In those cases, because we are using the error rate from the validation set to pick the best model, our validation error rate will be lower than the model's true error rate. Therefore, we need a set of data that we never used to optimize or pick our model to assess an unbiased error rate.\n",
    "\n",
    "[Learn More](https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets)\n",
    "\n",
    "Since, in this case, we are just using one model (no model selection), and once trained, we aren't tuning it any further (no Hyperparameter_optimization), we can skip the test set and use a 70/30 train/validate split.\n",
    "\n",
    "#### Model fit\n",
    "\n",
    "It is crucial to know how a model's prediction fits the actual predictions. If the model is not adequately trained, the training phase's predictions can't closely match the true predictions. This is called underfitting. In an underfit case, the model will also perform poorly on the evaluation set.\n",
    "\n",
    "On the other hand, if a model performs amazingly on the training set (think 99% accurate) but poorly on the validation set, it is most likely overfit. You can think of this as a model that has almost memorized the training set and therefore is incapable of performing well in predicting new inputs. \n",
    "\n",
    "If a model is neither underfitting nor overfit, it is called balanced. This is the place we want to be in, and this is why we keep unseen data (validation and test splits). We use those unseen data to detect if our model is overfitting the data.\n",
    "\n",
    "[Learn More](https://en.wikipedia.org/wiki/Overfitting)\n",
    "\n",
    "#### Uppercase `X` and lowercase `y`\n",
    "\n",
    "A tradition among data scientists is to use uppercase `X` to denote the input data to a model and the lowercase `y` the target predictions or values that the model is expected to learn and predict. That is, each set (train/validate/test) is split into `X` and `y`, where `y` is the predicted column, and `X` is all the other columns.\n",
    "\n",
    "With the definitions our of the way, let's split our data, perform our training, and see how they perform on unseen data (validation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Splitting Data for Training\n",
    "\n",
    "We begin by splitting our full dataset into `X` and `y` and then splitting it to train/validation subsets (also called splits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries we need\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Split target column our\n",
    "X = df.drop(\"survived\", axis = 1)\n",
    "y = df[\"survived\"]\n",
    "\n",
    "# perform the train/validation split\n",
    "# test is our validation here since we won't further split out train set\n",
    "# We specify the random state here for reprocucable results. \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=20)\n",
    "\n",
    "# Let's check the shapes (table dimentions) to make sure it makes sense\n",
    "X_train.shape, y_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise:\n",
    "# Take a look at the data splits [X_train, X_test, y_train, y_test] and see if they make sense to you. You can explore, length, columns, dtypes, ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Model Training\n",
    "\n",
    "With `X_train`, `y_train`, `X_valid`, and `y_valid` ready, we move on to training our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font colot='red'><b>Note:</b></font> \n",
    "The following line won't work, once you see the error, read the <b>very last line</b> and try to debug the issue in the next cell, then move forward to see the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "\n",
    "# Note: This won't work, once you see the error, read it try to debug the issue, then move forward to see the solution.\n",
    "decision_tree.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like something is wrong. Take a look at the last line of the ValueError message and use the cell below to explore and see if you can findout what is wrong. We left a small mistake intentionally to experience the debugging process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exrcise \n",
    "# Explore the copy of our original table below and find out what is wrong\n",
    "# Make sure not to modify to original df, and only work on the df2\n",
    "df2 = df.copy()\n",
    "\n",
    "# You may explore df2 here\n",
    "\n",
    "\n",
    "\n",
    "# Hint: There is a missing value. Can you find it? (A quick online search might help.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Solution:</b>\n",
    "\n",
    "As the error says:\n",
    "> Input contains NaN, infinity or a value too large for dtype('float32')\n",
    "\n",
    "So let's find the column with NaN and fill it with an average for that column.\n",
    "\n",
    "We will use the `.isnull()` to fill the table with true wherever there is a null, and then use the `.any()` to show if in each column there is any True (which would mean a `NaN`). You can run these steps one-by-one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's count the `NaN` in fare column. \n",
    "\n",
    "Note that `df[\"fare\"].isnull().count()` wouldn't work as it would count all values: `true` and `false`! Instead, use `.sum()` that treats `true` as 1 and `false` as 0, hence counting the `true`s by summing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"fare\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok there is only one missing value, so let's replace it with the mean and move on.abs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fare_mode = df[\"fare\"].mode()[0]\n",
    "df[\"fare\"] = df[\"fare\"].fillna(fare_mode)\n",
    "\n",
    "# and let's make sure there are no more nulls\n",
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect, no more nulls, we can now train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redo our split with fixed data\n",
    "X = df.drop(\"survived\", axis = 1)\n",
    "y = df[\"survived\"]\n",
    "\n",
    "# Note a method can have more than one output, here, it has 4!\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "# Decision Tree\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "\n",
    "# Train Model\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions for validation set\n",
    "y_pred = decision_tree.predict(X_test)\n",
    "\n",
    "# calculate models accuracy on training set\n",
    "train_score = decision_tree.score(X_train, y_train)\n",
    "test_score = decision_tree.score(X_test, y_test)\n",
    "\n",
    "# Print results\n",
    "# We know this is a new syntax to format and round numbers to two decimal places within a string. \n",
    "# You can learn more at https://www.w3schools.com/python/ref_string_format.asp\n",
    "print(\"Trainins Score: {:.2f} validation score: {:.2f}\".format(train_score, test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our decision tree is overfitting! It does very well with the training data 97% accuracy and performs poorly on unseen data (validation set). \n",
    "\n",
    "How to counter that is beyond this notebook's scope, but you can begin by forcing each branch of the tree to have at least 10 leaves. That is replacing the following line\n",
    "```python\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "```\n",
    "with:\n",
    "```python\n",
    "decision_tree = DecisionTreeClassifier(min_samples_leaf=10)\n",
    "```\n",
    "\n",
    "This should reduce overfitting and improve our accuracy to about 80%! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning the Tree Parameters\n",
    "decision_tree = DecisionTreeClassifier(min_samples_leaf=10)\n",
    "\n",
    "# Train Model\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions for validation set\n",
    "y_pred = decision_tree.predict(X_test)\n",
    "\n",
    "# calculate models accuracy on training set\n",
    "train_score = decision_tree.score(X_train, y_train)\n",
    "test_score = decision_tree.score(X_test, y_test)\n",
    "\n",
    "# Print results\n",
    "print(\"Trainins Score: {:.2f} validation score: {:.2f}\".format(train_score, test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This brings us to the end of this notebook. You now have created your first Machine Learning model with high accuracy and can apply your knowledge to your specific domain.\n",
    "\n",
    "You can always come back to this notebook and try different experiments by adding or editing cells.\n",
    "\n",
    "Thanks for joining us, and let us know if you have any comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Challenge\n",
    "\n",
    "It is possible to improve the model even further. By following the steps below, you can create an even better performing model while practicing the skills you learned earlier.\n",
    "\n",
    "<b>Intuition:</b> \n",
    "\n",
    "We consider the following hypothesis: those who were alone were less likely to survive. \n",
    "\n",
    "<b>Thought Process:</b> \n",
    "\n",
    "Based on our dataset's columns, we could deduce if someone was alone or not based on whether parents accompanied them or whether they had any siblings accompanying them. To create a new feature (column) that indicates this and use that in our model training. We can then compare the performance of our model to the model that we built earlier.\n",
    "\n",
    "<b>Approach:</b>\n",
    "\n",
    "1. Acquire the data\n",
    "1. Create a new feature (column) called `IsNotAlone`\n",
    "1. Fill in the new column with a boolean `True` or `False` indicating if they were not alone\n",
    "1. Split the new dataset into `X` and `y`\n",
    "1. Split both `X` and `y` into training and validation sets\n",
    "1. Train a new Decision Tree model based on this new dataset\n",
    "1. Evaluate the model's performance\n",
    "\n",
    "<b>Hint:</b>\n",
    "\n",
    "Once you have an idea on what to do, an online search of `how to do ... in pandas` or `how to do ... with scikit decision trees` might be all you need to find the missing link. And, of course, all the steps are also outlined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Acquire the data\n",
    "\n",
    "# Since we kept the sibsp or parch columns already, a copy of df in its current state works for us\n",
    "# use `df.copy()` to make a copy of it\n",
    "\n",
    "df2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 & 3. Set df2['IsNotAlone'] equal to a logical OR of sibsp or parch being greater than zero\n",
    "# Fill in the blanks\n",
    "\n",
    "df2['IsNotAlone'] = ( ) or ( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Split the survived column of df2 to y and keep the rest in X\n",
    "# fill in the blanks\n",
    "\n",
    "X = df2.drop( , axis = 1)\n",
    "y = df2[ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Use the train_test_split method to split your data\n",
    "# fill in the blanks\n",
    "\n",
    "\n",
    "X_train, X_test, y_valid, y_valid = train_test_split( ,  , test_size= )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Create a new DecisionTreeClassifier\n",
    "challenge_decision_tree = DecisionTreeClassifier(min_samples_leaf=10)\n",
    "\n",
    "# Train Model\n",
    "\n",
    "# Make predictions for validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Using .score(X_train, y_train) and .score(X_valid, y_valid) calculate your training and validation score\n",
    "\n",
    "\n",
    "# calculate models accuracy on training set\n",
    "\n",
    "# Print results (Done for you)\n",
    "print(\"Trainins Score: {:.2f} validation score: {:.2f}\".format(train_score, test_score))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
