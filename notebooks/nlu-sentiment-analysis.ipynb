{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python and Analytics workshop - Using Natural Language Understanding and Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this portion of the workshop, we'll use an instance of [Watson Natural Language Understanding](https://cloud.ibm.com/catalog/services/natural-language-understanding) to gather insights into data.\n",
    "\n",
    "Watson Natural Language Understanding is a cloud native product that uses deep learning to extract metadata from text such as entities, keywords, categories, sentiment, emotion, relations, and syntax.\n",
    "There is a rich [API](https://cloud.ibm.com/apidocs/natural-language-understanding?code=python) that we will use along with the [Watson Python SDK](https://github.com/watson-developer-cloud/python-sdk) to analyze our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [1.0 Setup - install modules](#setup)\n",
    "- [2.0 Test NLU APIs](#test)\n",
    "- [3.0 Import Data and Setup Pandas Dataframe ](#pandas)\n",
    "- [4.0 Clean and Prepare data for NLU scoring](#clean)\n",
    "- [5.0 Analyze response from NLU ](#analyze)\n",
    "- [6.0 Get sentiment by row](#sentiment-row)\n",
    "- [7.0 Graph with matplotlib](#graph)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Setup - Install Modules<a name=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the [Watson Python SDK](https://github.com/watson-developer-cloud/python-sdk) to access the [NLU APIs](https://cloud.ibm.com/apidocs/natural-language-understanding?code=python) programatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade numpy==1.16.4\n",
    "!pip install --upgrade pandas==1.0.5\n",
    "!pip install --upgrade ibm-watson==4.7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important: Restart the Jupyter kernel now\n",
    "Restart the kernal by going to the `Kernel` tab above and choosing `Restart`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import python modules from the Watson Python SDKs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ibm_watson import NaturalLanguageUnderstandingV1\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "from ibm_watson.natural_language_understanding_v1 import Features,CategoriesOptions,EmotionOptions,KeywordsOptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Add NLU credentials\n",
    "Get the [IAM Authentication Key](https://cloud.ibm.com/apidocs/natural-language-understanding?code=python#authentication) and [Service URL](https://cloud.ibm.com/apidocs/natural-language-understanding?code=python#service-endpoint) that you obtained when you [Created a Watson NLU instance](https://github.ibm.com/IBMDeveloper/python-and-analytics/tree/addNLU/workshop/natural-language-understanding#create-a-watson-nlu-instance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your [IAM Authentication Key](https://cloud.ibm.com/apidocs/natural-language-understanding?code=python#authentication) below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IAM_KEY = '**************'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your [NLU Service URL](https://cloud.ibm.com/apidocs/natural-language-understanding?code=python#service-endpoint) below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVICE_URL = '**************'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Test NLU APIs <a name=\"test\"></a>\n",
    "Run a quick check to make sure everything is working. We'll use a [basic web page](https://www.ibm.com) to see how Watson Natural Language Understanding can extract categories when given a URL. [This example](https://cloud.ibm.com/apidocs/natural-language-understanding?code=python#categories) comes from the Watson NLU documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authenticator = IAMAuthenticator(IAM_KEY)\n",
    "natural_language_understanding = NaturalLanguageUnderstandingV1(version='2020-08-01',authenticator=authenticator)\n",
    "\n",
    "natural_language_understanding.set_service_url(SERVICE_URL)\n",
    "\n",
    "response = natural_language_understanding.analyze(\n",
    "    url='www.ibm.com',\n",
    "    features=Features(categories=CategoriesOptions(limit=3))).get_result()\n",
    "\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Import Data and Setup Pandas Dataframe <a name=\"pandas\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read [cfpciti.csv](https://raw.githubusercontent.com/IBM/python-and-analytics/master/data/cfpbciti.csv) which contains data from the Consumer Credit Bureau for consumer complaints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/IBM/python-and-analytics/master/data/cfpbciti.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Clean and Prepare data for NLU scoring <a name=\"clean\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in the customer's sentement about various things, like `Product` or `Sub-product`. The column for `Customer complaint narrative` looks like it contains the text we should analyze. Let's look at this.\n",
    "The first few rows have a 'NaN' value, so we'll look at row 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = df.loc[3,\"Consumer complaint narrative\"]\n",
    "text1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's drop all the 'NaN' values found in the `Consumer complaint narrative` column using the Pandas method [dropna()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df['Consumer complaint narrative'].dropna(how = 'all')\n",
    "df2.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll convert the dataframe column to a string to send to the NLU endpoint for scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = df2.to_string()\n",
    "df_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll send this data to NLU to get [keywords](https://cloud.ibm.com/apidocs/natural-language-understanding?code=python#keywords), [sentiment](https://cloud.ibm.com/apidocs/natural-language-understanding?code=python#sentiment), and [emotions](https://cloud.ibm.com/apidocs/natural-language-understanding?code=python#emotion) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = natural_language_understanding.analyze(\n",
    "    text = df_text,\n",
    "    features=Features(keywords=KeywordsOptions(sentiment=True,emotion=True,limit=5))).get_result()\n",
    "\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmmmm...It looks like we've picked up the 'XX/XX' field for dates that are obscured. Since this is occuring frequently, the NLU scoring is tagging it with a high relevance score. Let's drop those 'XX' characters to get a better response from NLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 =df2.replace(regex=['X+'],value='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = df2.to_string()\n",
    "df_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now that those 'XX' characters are gone, let's score again with NLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = natural_language_understanding.analyze(\n",
    "    text = df_text,\n",
    "    features=Features(keywords=KeywordsOptions(sentiment=True,emotion=True,limit=5))).get_result()\n",
    "\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks like information we can use. We notice that there is a 50,000 character limit. We'll work with that in a bit. For now, let's see if we can analyze some of this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Analyze response from NLU <a name=analyze></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a dataframe from this API response. First, we'll look at the part of the response json that is associated with the key 'keywords`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respj = json.dumps(response['keywords'])\n",
    "respj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK. That looks good. Now, let's create a Pandas dataframe with that json using the method [read_json()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_json.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df = pd.read_json(respj)\n",
    "json_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That kinda worked. But the `Sentiment` column is composed of json that has multiple values in a [Python dict](https://docs.python.org/3/tutorial/datastructures.html#dictionaries) form, as does the `emotion` column.\n",
    "We can use [json_normalize()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html) to create a dataframe that splits up `sentiment` and `emotion`. We'll drop a few columns and focus only on the `sentiment.score` and the `emotion.*` features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_df = pd.json_normalize(response['keywords'])\n",
    "norm_df.drop('relevance',inplace = True, axis = 1)\n",
    "norm_df.drop('count',inplace = True, axis = 1)\n",
    "norm_df.drop('sentiment.label',inplace = True, axis = 1)\n",
    "norm_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exploration of the data gives us some tools to work with. We'll continue to analyze the text in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0 Get sentiment by row <a name=\"sentiment-row\"></a>\n",
    "Now, let's derive some sentiment and emotion information on a per-row basis, to provide more granualarity.\n",
    "The number of API calls that you can make to Watson NLU is [rate limited and dependent on your service plan](https://cloud.ibm.com/catalog/services/natural-language-understanding), so in order to limit the number of API calls to the NLU endpoint we'll start with just 50 rows by setting `num_rows` to 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rows = df.head(num_rows)\n",
    "df_rows = df_rows.dropna(subset=['Consumer complaint narrative'],how = 'any')\n",
    "df_rows =df_rows.replace(regex=['X+'],value='')\n",
    "df_rows.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that when we dropped the rows with a `NaN` value for `Customer complaint narrative`, the indexes are no longer sequential. Let's use [reset_index](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reset_index.html) to fix this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rows.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to iterate through the rows for a Pandas dataframe. We'll use [iterrows()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iterrows.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have a date for these entries. Let's put them into [Pandas datetime](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html) format. We can use this later to do time series graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_rows.iterrows():\n",
    "    df_rows.loc[index,'Date received'] = datetime.strptime(row['Date received'], \"%m/%d/%y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look for something that we can use with Watson NLU to derive an analysis of the sentiment of the customer feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rows.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df_rows['Consumer complaint narrative'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks like what we want. Now, we'll create a list to hold the `responses`, call Watson NLU with the data and then populate the responses list. We'll do the same with a list called `normalize` that we can use along with [json_normalize()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "normalize = []\n",
    "for index, row in df_rows.iterrows():\n",
    "    \n",
    "    response = natural_language_understanding.analyze(\n",
    "    text = row['Consumer complaint narrative'],\n",
    "    features=Features(keywords=KeywordsOptions(sentiment=True,emotion=True,limit=1))).get_result()\n",
    "    normalize.append(pd.json_normalize(response['keywords']))\n",
    "    responses.append(response)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the `responses` list and the `normalize` to the df_rows dataframe. We can continue to use these new data features, but more commonly we'll derive new dataframes for our experiments and change those new dataframes instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rows['response'] = responses\n",
    "df_rows.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rows['normalized'] = normalize\n",
    "df_rows.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new dataframe where we can pull out the column for the `emotion` `anger`, then sort by the highest rating of `anger`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in test_df.iterrows():\n",
    "    test_df.loc[index,\"anger\"] = test_df.iloc[index]['response']['keywords'][0]['emotion']['anger']\n",
    "    test_df.loc[index,\"sentiment.score\"] = test_df.iloc[index]['response']['keywords'][0]['sentiment']['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll look for entries that rank high in `anger`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = test_df.sort_values(by='anger', ascending=False)\n",
    "sorted_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the `Consumer complaint narrative` that causes the most anger (the one at the top of the sorted list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df.iloc[0]['Consumer complaint narrative']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could to the same for other entries that rank high in anger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df.iloc[1]['Consumer complaint narrative']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at those with the highest negative sentiment. Note that for this, we'll sort by \"ascending\", since the more negative numbers represent a higher degree of negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = test_df.sort_values(by='sentiment.score', ascending=True)\n",
    "sorted_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df.iloc[0]['Consumer complaint narrative']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it's not a surprise that the entry with the largest negative `sentiment.score` also has the highest rating for `anger`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.0 Graph with matplotlib <a name=\"graph\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some graphs using [matplotlib](https://matplotlib.org). You may wish to explore more details about the Jupyter notebook [magic functions](https://ipython.readthedocs.io/en/stable/interactive/tutorial.html#magics-explained) that you see used with the command `%matplotlib inline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Time series graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see if there is anything interesting when we plot data against time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df.plot(kind='line',x='Date received',y='anger',color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted_df.plot(kind='line',x='Date received',y='sentiment.score',color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot both `sentiment.score` and `anger` against time to look for correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df.plot(kind='line',x='Date received',y=['sentiment.score','anger'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Bar graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sum up the number of times a given `Product` or `Sub-product` appears using the [Python collections library Counter](https://docs.python.org/2/library/collections.html#collections.Counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll create a bar graph to see which `Product` are refered to in the most customer complaints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_hist = Counter(sorted_df['Product'].replace('\\n', ''))\n",
    "\n",
    "counts = bar_hist.values()\n",
    "letters = bar_hist.keys()\n",
    "\n",
    "# graph data\n",
    "bar_x_locations = np.arange(len(counts))\n",
    "plt.bar(bar_x_locations, counts, align = 'center')\n",
    "plt.xticks(bar_x_locations, letters, rotation=90)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same for `Sub-product`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bar_hist = Counter(sorted_df['Sub-product'].replace('\\n', ''))\n",
    "\n",
    "counts = bar_hist.values()\n",
    "letters = bar_hist.keys()\n",
    "\n",
    "# graph data\n",
    "bar_x_locations = np.arange(len(counts))\n",
    "plt.bar(bar_x_locations, counts, align = 'center')\n",
    "plt.xticks(bar_x_locations, letters, rotation=90)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Scatterplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use some matplotlib and numpy code to generate a 3D scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mpl_toolkits.mplot3d.axes3d as axes3d\n",
    "\n",
    "\n",
    "Xuniques, X = np.unique(sorted_df['Sub-product'], return_inverse=True)\n",
    "Yuniques, Y = np.unique(sorted_df['Product'], return_inverse=True)\n",
    "Z= sorted_df['anger']\n",
    "fig = plt.figure(figsize= [15,8])\n",
    "ax = fig.add_subplot(1, 1, 1, projection='3d',autoscale_on=True)\n",
    "ax.scatter(X, Y, Z, s=10, c='b')\n",
    "ax.set(xticks=range(len(Xuniques)), xticklabels=Xuniques,\n",
    "       yticks=range(len(Yuniques)), yticklabels=Yuniques)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
